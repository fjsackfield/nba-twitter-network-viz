{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MGT 8803 Project\n",
    "## Basketball Reference Web Scraper\n",
    "\n",
    "### Fred Sackfield\n",
    "\n",
    "This notebook contains code to scrape twitter handle data from player pages on basketball-reference.com. These twitter handles will be stored in a csv file and used as input for an R script that collects twitter data for each player. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.error import URLError, HTTPError\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from time import sleep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soup(url):\n",
    "    try:\n",
    "        page = requests.get(url, timeout=10)\n",
    "    except HTTPError as e:\n",
    "        print('ERROR CODE: ',e.code)\n",
    "        return None\n",
    "    except URLError as e:\n",
    "        print('ERROR REASON: ',e.reason)\n",
    "        return None\n",
    "\n",
    "    html = page.text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    return soup\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Step 1 - create a list of all team-specific urls\n",
    "\n",
    "home_url = 'https://www.basketball-reference.com'\n",
    "\n",
    "home_soup = create_soup(home_url)\n",
    "\n",
    "team_hrefs = []\n",
    "\n",
    "for team in home_soup.find_all('th',{'data-stat':'team_name'}):\n",
    "    \n",
    "    if 'aria-label' not in team.attrs.keys():\n",
    "        team_hrefs.append(team.find('a').attrs['href'])\n",
    "\n",
    "len(team_hrefs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected players from MIL\n",
      "Collected players from TOR\n",
      "Collected players from BOS\n",
      "Collected players from MIA\n",
      "Collected players from IND\n",
      "Collected players from PHI\n",
      "Collected players from BRK\n",
      "Collected players from ORL\n",
      "Collected players from WAS\n",
      "Collected players from CHO\n",
      "Collected players from CHI\n",
      "Collected players from NYK\n",
      "Collected players from DET\n",
      "Collected players from ATL\n",
      "Collected players from CLE\n",
      "Collected players from LAL\n",
      "Collected players from LAC\n",
      "Collected players from DEN\n",
      "Collected players from UTA\n",
      "Collected players from OKC\n",
      "Collected players from HOU\n",
      "Collected players from DAL\n",
      "Collected players from MEM\n",
      "Collected players from POR\n",
      "Collected players from NOP\n",
      "Collected players from SAC\n",
      "Collected players from SAS\n",
      "Collected players from PHO\n",
      "Collected players from MIN\n",
      "Collected players from GSW\n"
     ]
    }
   ],
   "source": [
    "## store the player data in a list of dictionaries, which have the following keys:\n",
    "## id, fullname, team, age, exp, college, handle\n",
    "\n",
    "players = []\n",
    "\n",
    "for href in team_hrefs:\n",
    "    sleep(2)\n",
    "    team_soup = create_soup(home_url+href)\n",
    "    roster = team_soup.find('div',{'id':'div_roster'}).find('tbody')\n",
    "    \n",
    "    for player in roster.find_all('tr'):\n",
    "\n",
    "        pref = player.find('a').attrs['href']\n",
    "        #print(pref)\n",
    "        player_soup = create_soup(home_url+pref)\n",
    "        \n",
    "        #filter out players who don't have Twitter or who play < 10mpg\n",
    "        if player_soup.find('tr',{'id':'per_game.2020'}) is None:\n",
    "            continue\n",
    "        elif float(player_soup.find('tr',{'id':'per_game.2020'}).find('td',{'data-stat':'mp_per_g'}).text) < 10:\n",
    "            continue\n",
    "        elif player_soup.find(text='Twitter') is None:\n",
    "            continue\n",
    "        else:\n",
    "            \n",
    "            player_cols = player.find_all('td')\n",
    "            bday = datetime.strptime(player_cols[4].text, \"%B %d, %Y\")\n",
    "            today = datetime.strptime(datetime.today().strftime(\"%B %d, %Y\"), \"%B %d, %Y\")\n",
    "\n",
    "            player_dict = {}\n",
    "            player_dict['id'] = pref.split('/')[3].split('.')[0]\n",
    "            player_dict['fullname'] = player.find('a').text\n",
    "            player_dict['team'] = href.split('/')[2]\n",
    "            player_dict['age'] = int(abs((today - bday).days)/365)\n",
    "            player_dict['exp'] = int(player_cols[6].text) if player_cols[6].text != 'R' else 0\n",
    "            player_dict['college'] = player_cols[7].text\n",
    "            player_dict['handle'] = player_soup.find(text='Twitter').find_next('a').text\n",
    "\n",
    "\n",
    "            players.append(player_dict)\n",
    "    print('Collected players from '+href.split('/')[2])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383\n"
     ]
    }
   ],
   "source": [
    "print(len(players))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data cleaning - if player attended multiple colleges, take the most recent one\n",
    "for p in players:\n",
    "    if len(p['college'].split(',')) > 1:\n",
    "        p['college'] = p['college'].split(',')[len(p['college'].split(','))-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write player data to a csv file\n",
    "\n",
    "player_columns = ['id','fullname','team','age','exp','college','handle']\n",
    "\n",
    "csv_file = \"players.csv\"\n",
    "try:\n",
    "    with open(csv_file, 'w', newline='\\n', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=player_columns)\n",
    "        writer.writeheader()\n",
    "        for data in players:\n",
    "            writer.writerow(data)\n",
    "except IOError:\n",
    "    print(\"I/O error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
